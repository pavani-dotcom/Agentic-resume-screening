# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tVee-4gVdj-7FlORzj4n7fTqN2qa7_hY
"""

!pip install -q groq PyPDF2

import os
import json
from groq import Groq
from google.colab import userdata

os.environ["GROQ_API_KEY"] = userdata.get("GROQ_API_KEY")
client = Groq(api_key=os.environ["GROQ_API_KEY"])

def call_llm(prompt: str) -> str:
    response = client.chat.completions.create(
        messages=[{"role": "user", "content": prompt}],
        model="llama-3.1-8b-instant",  # âœ… Updated model name
        temperature=0.0
    )
    return response.choices[0].message.content

import PyPDF2

def parse_resume(file_path: str) -> str:
    try:
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception as e:
        return f"ERROR_PARSING_RESUME: {str(e)}"

def resume_parser_agent(resume_text: str) -> dict:
    if "ERROR_PARSING_RESUME" in resume_text:
        return {"candidate_profile": {}, "parsing_error": True}

    prompt = f"""
Extract structured info from this resume. Return ONLY JSON with these keys:
{{"name": "...", "years_experience": 0, "skills": ["..."], "education": "...", "domain_experience": ["..."]}}

Resume:
{resume_text}
"""
    try:
        text = call_llm(prompt)
        print("ğŸ“ Resume LLM Response:", text[:300])  # DEBUG

        # Extract JSON robustly
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1:
            text = text[start:end+1]

        # Clean common issues
        text = text.replace("\n", "").replace("\\", "").strip()
        profile = json.loads(text)
        return {"candidate_profile": profile, "parsing_error": False}
    except Exception as e:
        print("âŒ Resume Parsing Error:", e)
        # Fallback structured data
        return {"candidate_profile": {
            "name": "Unknown",
            "years_experience": 0,
            "skills": [],
            "education": "Not extracted",
            "domain_experience": []
        }, "parsing_error": False}  # Continue processing

def jd_parser_agent(jd_text: str) -> dict:
    prompt = f"""
Extract job requirements. Return ONLY JSON with these keys:
{{"role": "...", "required_skills": ["..."], "required_experience_years": 0, "strictness_level": "strict"|"flexible"|"vague"}}

Job Description:
{jd_text}
"""
    try:
        text = call_llm(prompt)
        print("ğŸ“ JD LLM Response:", text[:300])  # DEBUG

        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1:
            text = text[start:end+1]

        text = text.replace("\n", "").replace("\\", "").strip()
        reqs = json.loads(text)
        return {"job_requirements": reqs}
    except Exception as e:
        print("âŒ JD Parsing Error:", e)
        return {"job_requirements": {
            "role": "Unknown",
            "required_skills": [],
            "required_experience_years": 0,
            "strictness_level": "vague"
        }}

def match_agent(state: dict) -> dict:
    profile = state.get("candidate_profile", {})
    job = state.get("job_requirements", {})

    # Skill matching
    cand_skills = set(s.lower() for s in profile.get("skills", []))
    req_skills = set(s.lower() for s in job.get("required_skills", []))
    matched = cand_skills & req_skills
    skill_score = len(matched) / max(1, len(req_skills))

    # Experience matching
    cand_exp = profile.get("years_experience", 0)
    req_exp = job.get("required_experience_years", 0)
    exp_score = min(cand_exp / req_exp, 1.0) if req_exp > 0 else 0.7

    match_score = round(0.6 * skill_score + 0.4 * exp_score, 2)
    strictness = job.get("strictness_level", "flexible")

    # Decision logic
    if strictness == "vague":
        requires_human, recommendation, confidence = True, "Needs manual review", 0.7
    elif strictness == "strict":
        if match_score >= 0.85:
            requires_human, recommendation, confidence = False, "Proceed to interview", 0.95
        elif match_score >= 0.3:
            requires_human, recommendation, confidence = True, "Needs manual review", 0.75
        else:
            requires_human, recommendation, confidence = False, "Reject", 0.90
    else:  # flexible
        if match_score >= 0.75:
            requires_human, recommendation, confidence = False, "Proceed to interview", 0.90
        elif match_score >= 0.5:
            requires_human, recommendation, confidence = True, "Needs manual review", 0.70
        else:
            requires_human, recommendation, confidence = False, "Reject", 0.85

    reasoning = (
        f"Match score {match_score}. Matched skills: {list(matched)}. "
        f"Experience: {cand_exp}y vs {req_exp}y required. Strictness: {strictness}."
    )

    return {
        "match_score": match_score,
        "confidence": confidence,
        "requires_human": requires_human,
        "recommendation": recommendation,
        "reasoning_summary": reasoning
    }

from google.colab import files

def validate_output(resume_name: str, jd_name: str, result: dict):
    print(f"\nğŸ” VALIDATION: {resume_name} + {jd_name}")
    print(f"âœ… Match Score: {result['match_score']}")
    print(f"âœ… Requires Human: {result['requires_human']}")
    print(f"âœ… Reasoning: {result['reasoning_summary'][:120]}...")

    if "vague" in jd_name and not result["requires_human"]:
        print("âŒ FAILED: Vague JD must require human review!")
    else:
        print("âœ… PASSED all validation checks")

# Upload files
print("ğŸ“¤ Upload resume (PDF):")
resume_file = list(files.upload().keys())[0]
print("ğŸ“„ Upload job description (TXT):")
jd_file = list(files.upload().keys())[0]

# Agentic flow
state = {}
state.update(resume_parser_agent(parse_resume(resume_file)))

if not state.get("parsing_error"):
    with open(jd_file, 'r') as f:
        state.update(jd_parser_agent(f.read()))

result = match_agent(state)

final_output = {
    "match_score": float(result["match_score"]),
    "recommendation": result["recommendation"],
    "requires_human": bool(result["requires_human"]),
    "confidence": float(result["confidence"]),
    "reasoning_summary": result["reasoning_summary"]
}

validate_output(resume_file, jd_file, final_output)
print("\n" + "="*50)
print("FINAL OUTPUT (Pitcrew Format):")
print(json.dumps(final_output, indent=2))

from google.colab import files

def validate_output(resume_name: str, jd_name: str, result: dict):
    print(f"\nğŸ” VALIDATION: {resume_name} + {jd_name}")
    print(f"âœ… Match Score: {result['match_score']}")
    print(f"âœ… Requires Human: {result['requires_human']}")
    print(f"âœ… Reasoning: {result['reasoning_summary'][:120]}...")

    if "vague" in jd_name and not result["requires_human"]:
        print("âŒ FAILED: Vague JD must require human review!")
    else:
        print("âœ… PASSED all validation checks")

# Upload files
print("ğŸ“¤ Upload resume (PDF):")
resume_file = list(files.upload().keys())[0]
print("ğŸ“„ Upload job description (TXT):")
jd_file = list(files.upload().keys())[0]

# Agentic flow
state = {}
state.update(resume_parser_agent(parse_resume(resume_file)))

if not state.get("parsing_error"):
    with open(jd_file, 'r') as f:
        state.update(jd_parser_agent(f.read()))

result = match_agent(state)

final_output = {
    "match_score": float(result["match_score"]),
    "recommendation": result["recommendation"],
    "requires_human": bool(result["requires_human"]),
    "confidence": float(result["confidence"]),
    "reasoning_summary": result["reasoning_summary"]
}

validate_output(resume_file, jd_file, final_output)
print("\n" + "="*50)
print("FINAL OUTPUT (Pitcrew Format):")
print(json.dumps(final_output, indent=2))

from google.colab import files

def validate_output(resume_name: str, jd_name: str, result: dict):
    print(f"\nğŸ” VALIDATION: {resume_name} + {jd_name}")
    print(f"âœ… Match Score: {result['match_score']}")
    print(f"âœ… Requires Human: {result['requires_human']}")
    print(f"âœ… Reasoning: {result['reasoning_summary'][:120]}...")

    if "vague" in jd_name and not result["requires_human"]:
        print("âŒ FAILED: Vague JD must require human review!")
    else:
        print("âœ… PASSED all validation checks")

# Upload files
print("ğŸ“¤ Upload resume (PDF):")
resume_file = list(files.upload().keys())[0]
print("ğŸ“„ Upload job description (TXT):")
jd_file = list(files.upload().keys())[0]

# Agentic flow
state = {}
state.update(resume_parser_agent(parse_resume(resume_file)))

if not state.get("parsing_error"):
    with open(jd_file, 'r') as f:
        state.update(jd_parser_agent(f.read()))

result = match_agent(state)

final_output = {
    "match_score": float(result["match_score"]),
    "recommendation": result["recommendation"],
    "requires_human": bool(result["requires_human"]),
    "confidence": float(result["confidence"]),
    "reasoning_summary": result["reasoning_summary"]
}

validate_output(resume_file, jd_file, final_output)
print("\n" + "="*50)
print("FINAL OUTPUT (Pitcrew Format):")
print(json.dumps(final_output, indent=2))

from google.colab import files

def validate_output(resume_name: str, jd_name: str, result: dict):
    print(f"\nğŸ” VALIDATION: {resume_name} + {jd_name}")
    print(f"âœ… Match Score: {result['match_score']}")
    print(f"âœ… Requires Human: {result['requires_human']}")
    print(f"âœ… Reasoning: {result['reasoning_summary'][:120]}...")

    if "vague" in jd_name and not result["requires_human"]:
        print("âŒ FAILED: Vague JD must require human review!")
    else:
        print("âœ… PASSED all validation checks")

# Upload files
print("ğŸ“¤ Upload resume (PDF):")
resume_file = list(files.upload().keys())[0]
print("ğŸ“„ Upload job description (TXT):")
jd_file = list(files.upload().keys())[0]

# Agentic flow
state = {}
state.update(resume_parser_agent(parse_resume(resume_file)))

if not state.get("parsing_error"):
    with open(jd_file, 'r') as f:
        state.update(jd_parser_agent(f.read()))

result = match_agent(state)

final_output = {
    "match_score": float(result["match_score"]),
    "recommendation": result["recommendation"],
    "requires_human": bool(result["requires_human"]),
    "confidence": float(result["confidence"]),
    "reasoning_summary": result["reasoning_summary"]
}

validate_output(resume_file, jd_file, final_output)
print("\n" + "="*50)
print("FINAL OUTPUT (Pitcrew Format):")
print(json.dumps(final_output, indent=2))

